{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install --quiet requests beautifulsoup4"
      ],
      "metadata": {
        "id": "jKyscAAX2jN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "def fetch_soup(url):\n",
        "    \"\"\"\n",
        "    Fetches and returns BeautifulSoup object from the given URL.\n",
        "    \"\"\"\n",
        "    response = requests.get(url)\n",
        "    return BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "def remove_unwanted_elements(content_div):\n",
        "    \"\"\"\n",
        "    Removes unwanted elements from the content div.\n",
        "    \"\"\"\n",
        "    if content_div:\n",
        "        #remove unwanted div with class 'vector-column-start'\n",
        "        vector_column_start = content_div.find('div', class_='vector-column-start')\n",
        "        if vector_column_start:\n",
        "            vector_column_start.decompose()\n",
        "\n",
        "        #remove content from the 'Notes' section\n",
        "        notes_section = content_div.find('span', id='Notes')\n",
        "        if notes_section:\n",
        "            for elem in notes_section.find_all_next():\n",
        "                elem.decompose()\n",
        "            notes_section.decompose()\n",
        "\n",
        "        #remove content from the 'References' section\n",
        "        references_section = content_div.find('span', id='References')\n",
        "        if references_section:\n",
        "            for elem in references_section.find_all_next():\n",
        "                elem.decompose()\n",
        "            references_section.decompose()\n",
        "\n",
        "        #remove all tables\n",
        "        tables = content_div.find_all('table')\n",
        "        for table in tables:\n",
        "            table.decompose()\n",
        "\n",
        "        #remove content from the specific hatnote div\n",
        "        hatnote_div = content_div.find('div', {'role': 'note', 'class': 'hatnote navigation-not-searchable'})\n",
        "        if hatnote_div:\n",
        "            hatnote_div.decompose()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Preprocesses the text content.\n",
        "    \"\"\"\n",
        "    text = \"\\n\".join([line.strip() for line in text.splitlines() if line.strip()])\n",
        "    text = text.replace('\\n', ' ').replace('\\t', ' ')\n",
        "    text = re.sub(r'\\[\\w+\\]', '', text)\n",
        "    text = re.sub(r'[\\u00A0]', ' ', text)\n",
        "    #remove line breaks within paragraphs\n",
        "    text = re.sub(r'\\n(\\S)', r' \\1', text)\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "def process_links(links):\n",
        "    \"\"\"\n",
        "    Processes each link and extracts text content.\n",
        "    \"\"\"\n",
        "    base_url = \"https://en.wikipedia.org\"\n",
        "    all_processed_text = []\n",
        "\n",
        "    for link in links:\n",
        "        href = link['href']\n",
        "        full_url = base_url + href\n",
        "\n",
        "        #fetch soup for the link\n",
        "        link_soup = fetch_soup(full_url)\n",
        "\n",
        "        #extract the content div\n",
        "        content_div = link_soup.find('div', {'id': 'mw-content-text', 'class': 'mw-body-content'})\n",
        "\n",
        "        #remove unwanted elements\n",
        "        remove_unwanted_elements(content_div)\n",
        "\n",
        "        #extract and preprocess the text content\n",
        "        if content_div:\n",
        "            text_content = content_div.get_text(separator='\\n')\n",
        "            processed_text = preprocess_text(text_content)\n",
        "\n",
        "            #append processed text to the cumulative string\n",
        "            all_processed_text.append(processed_text.strip())\n",
        "\n",
        "    return all_processed_text"
      ],
      "metadata": {
        "id": "Y1gJp6Ph2jKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "complete_list = []\n",
        "urls = [\"https://en.wikipedia.org/wiki/List_of_American_films_of_2024\",\n",
        "        \"https://en.wikipedia.org/wiki/List_of_Malayalam_films_of_2024\"]\n",
        "\n",
        "for url in urls:\n",
        "  soup = fetch_soup(url)\n",
        "\n",
        "  #find all href links in the table\n",
        "  table = soup.find('table', class_='wikitable')\n",
        "  links = table.find_all('a', href=True)\n",
        "\n",
        "  #process each link and get all processed text\n",
        "  processed_text = process_links(links)\n",
        "  complete_list.append(processed_text)"
      ],
      "metadata": {
        "id": "HXyHCzUV2jAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#save extracted content to txt file\n",
        "with open('movie_data.txt', 'w') as f:\n",
        "  for language in complete_list:\n",
        "    for movie in language:\n",
        "      f.write(movie)\n",
        "      f.write('\\n\\n')\n"
      ],
      "metadata": {
        "id": "d-lAj5tdHf1K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}